{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRCVPUgPCDyz"
   },
   "source": [
    "# exemplo_nb_base\n",
    "\n",
    "Aqui é recomendado adicionar um comentário explicando o contexto do notebook, tabelas e colunas que são adicionadas, e uma descrição sobre a tabela de saída.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delPYogbCmhN"
   },
   "source": [
    "## Configurações Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUMbO9XqCouN"
   },
   "source": [
    "### Preparação do Ambiente PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKufl5nnCsMT"
   },
   "source": [
    "#### Instalando as Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 37039,
     "status": "ok",
     "timestamp": 1754229737556,
     "user": {
      "displayName": "Otávio Maldaner",
      "userId": "03427891421355464378"
     },
     "user_tz": 180
    },
    "id": "aN17UfVfBijw"
   },
   "outputs": [],
   "source": [
    "# Instala o Java 8\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Baixa o Apache Spark 3.4.1 com Hadoop 3 (versão estável e recente)\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
    "\n",
    "# Descompacta o arquivo baixado\n",
    "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
    "\n",
    "# Instala a biblioteca findspark\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kACGaKNC3iU"
   },
   "source": [
    "#### Editando as Variáveis de Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 10012,
     "status": "ok",
     "timestamp": 1754229747571,
     "user": {
      "displayName": "Otávio Maldaner",
      "userId": "03427891421355464378"
     },
     "user_tz": 180
    },
    "id": "DT_vi-FsCy8e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define a variável de ambiente do Java\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "\n",
    "# Define a variável de ambiente do Spark\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cria a SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"E-commerce Dataset\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwa6n7yyC_Tz"
   },
   "source": [
    "### Importando as Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1754229747573,
     "user": {
      "displayName": "Otávio Maldaner",
      "userId": "03427891421355464378"
     },
     "user_tz": 180
    },
    "id": "NQmrmaqKC7h7"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmCwwQYkDD8g"
   },
   "source": [
    "### Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 941,
     "status": "ok",
     "timestamp": 1754229825378,
     "user": {
      "displayName": "Otávio Maldaner",
      "userId": "03427891421355464378"
     },
     "user_tz": 180
    },
    "id": "yg72ehHxDCPz"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Carrega o dataset de clientes do Olist. Esse é um exemplo de importação de dados \n",
    "de um arquivo CSV, porém, normalmente é utilizado o comando spark.read.table,\n",
    "para ingerir tabelas de um data lake ou data warehouse.\n",
    "\n",
    "Não é recomendado fazer grandes transformações nessa célula,\n",
    "pois o objetivo é apenas carregar o dataset, \n",
    "fazendo o select das colunas que serão utilizadas ou o drop de colunas inúteis.\n",
    "\"\"\" \n",
    "df_customers_dataset = (\n",
    "    spark.read.csv(\n",
    "        \"/content/olist_customers_dataset.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .drop(\"customer_city\", \"customer_state\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dessa célula é possível fazer diversas transformações e cálculos em cima das tabelas importadas. Normalmente são feitos joins entre tabelas e a aplicação de alguns cálculos com base nas colunas que foram adicionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lhoSC0IEBdX"
   },
   "source": [
    "## Salvamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui é feita a modelagem dos dados, onde é definido o schema final da tabela que será escrita, criação de relações entre as demais tabelas e adição de descrição de colunas e tabelas.\n",
    "\n",
    "### Observação Importante:\n",
    "Nessa oficina não temos um data lake para armazenar os dados, então essa etapa da modelagem deve ser ignorada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS TABLE dim_clientes (\n",
    "  customer_id STRING NOT NULL COMMENT \"ID único do cliente, associado a um pedido específico. Chave primária.\",\n",
    "  customer_zip_code_prefix STRING COMMENT \"CEP de cadastro do cliente.\",\n",
    "  client_name STRING COMMENT \"Nome do cliente.\",\n",
    "  CONSTRAINT fk_dim_clientes_geo FOREIGN KEY(customer_zip_code_prefix) REFERENCES dim_geolocalizacao(geolocation_zip_code_prefix)\n",
    ")\n",
    "COMMENT \"Tabela de dimensão de clientes do Olist, contendo informações básicas dos clientes.\"\n",
    "CLUSTER BY (customer_id);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, a escrita da tabela final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4h6UAsYuEDJh"
   },
   "outputs": [],
   "source": [
    "# Exemplo de escrita da tabela final em um data lake\n",
    "(\n",
    "    df_customers_dataset\n",
    "    .write.mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(\"ofc_dev_master.clients.dim_clientes\")\n",
    ")\n",
    "\n",
    "# Exemplo de um arquivo CSV, para o caso da oficina\n",
    "(\n",
    "    df_customers_dataset\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .option(\"header\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .csv(\"/transformed_data/dim_clientes\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Célula utilizada para o download do CSV gerado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from google.colab import files\n",
    "output_dir = \"/transformed_data/dim_clientes\"\n",
    "\n",
    "try:\n",
    "    # Encontra o arquivo CSV específico criado pelo Spark\n",
    "    csv_file_path = glob.glob(f\"{output_dir}/part-*.csv\")[0]\n",
    "    print(f\"Arquivo encontrado para download: {csv_file_path}\")\n",
    "\n",
    "    # Usa a biblioteca do Colab para iniciar o download\n",
    "    print(\"Iniciando o download...\")\n",
    "    files.download(csv_file_path)\n",
    "    print(\"Download concluído!\")\n",
    "\n",
    "except IndexError:\n",
    "    print(f\"Erro: Nenhum arquivo CSV foi encontrado no diretório '{output_dir}'.\")\n",
    "    print(\"Verifique se o passo de escrita do Spark foi executado com sucesso.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMMNPemZiZfuUMBYnRGewB7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
